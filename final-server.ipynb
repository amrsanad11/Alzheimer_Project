{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":140443,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":118955,"modelId":142204}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastapi uvicorn python-multipart Pillow torch torchvision\n!pip install pyngrok","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:41:29.274246Z","iopub.execute_input":"2025-05-12T21:41:29.274544Z","iopub.status.idle":"2025-05-12T21:41:47.240061Z","shell.execute_reply.started":"2025-05-12T21:41:29.274514Z","shell.execute_reply":"2025-05-12T21:41:47.239183Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (0.111.0)\nRequirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (0.30.1)\nRequirement already satisfied: python-multipart in /opt/conda/lib/python3.10/site-packages (0.0.9)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (10.3.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.37.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from fastapi) (2.9.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (4.12.2)\nRequirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.0.4)\nRequirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.27.0)\nRequirement already satisfied: jinja2>=2.11.2 in /opt/conda/lib/python3.10/site-packages (from fastapi) (3.1.4)\nRequirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi) (5.10.0)\nRequirement already satisfied: orjson>=3.2.1 in /opt/conda/lib/python3.10/site-packages (from fastapi) (3.10.4)\nRequirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi) (2.1.1)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi) (2.6.1)\nRequirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi) (3.7)\nRequirement already satisfied: typer>=0.12.3 in /opt/conda/lib/python3.10/site-packages (from fastapi-cli>=0.0.2->fastapi) (0.12.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi) (2.1.5)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (1.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (6.0.2)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (0.22.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi) (12.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.0)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\nCollecting pyngrok\n  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.2)\nDownloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.2.8\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# final ","metadata":{}},{"cell_type":"code","source":"with open('main.py', 'w') as f:\n    f.write('''\nfrom fastapi import FastAPI, UploadFile, File\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\nimport torch\nfrom PIL import Image\nimport io\nfrom torchvision import transforms\n\napp = FastAPI()\n\n# Enable CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Replace with specific domains in production\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Load the TorchScript model\nmodel = torch.jit.load(\"/kaggle/input/final_model/pytorch/default/1/model_final.pt\")\ndevice = torch.device(\"cpu\")  # Force to CPU\nmodel.to(device)\nmodel.eval()\n\n# Define the image preprocessing function\ndef get_transforms():\n    return transforms.Compose([\n        transforms.Resize((248, 248)),\n        transforms.ToTensor(),\n    ])\n\ndef preprocess_and_predict(model, img_tensor):\n    img_tensor = img_tensor.unsqueeze(0).to(device)  # Add batch dimension\n\n    # Compute mean and std\n    mean = torch.mean(img_tensor)\n    std = torch.std(img_tensor)\n\n    # Make prediction\n    with torch.no_grad():\n        output = model(img_tensor, mean.unsqueeze(0).to(device), std.unsqueeze(0).to(device))\n        predicted_class = output.argmax(dim=1).item()\n\n    return predicted_class, mean.item(), std.item()\n\n# Define the response model\nclass PredictionResponse(BaseModel):\n    predicted_class: int\n    mean: float\n    std: float\n    description: str\n\n@app.post(\"/predict/\", response_model=PredictionResponse)\nasync def predict_image(file: UploadFile = File(...)):\n    # Read image file\n    image_bytes = await file.read()\n    image = Image.open(io.BytesIO(image_bytes)).convert('L')\n\n    # Preprocess the image\n    transform = get_transforms()\n    img_tensor = transform(image)\n\n    # Run the prediction\n    predicted_class, mean, std = preprocess_and_predict(model, img_tensor)\n\n    # Map predicted class to dementia type\n    dementia_types = {0: \"Mild Dementia\", 1: \"Moderate Dementia\", 2: \"Non Demented\", 3: \"Very Mild Dementia\"}\n    description = dementia_types.get(predicted_class, \"Not Supported\")\n\n    return PredictionResponse(\n        predicted_class=predicted_class,\n        mean=mean,\n        std=std,\n        description=description\n    )\n\n# Add a root route\n@app.get(\"/\")\nasync def read_root():\n    return {\"message\": \"Welcome to the Dementia Prediction API. Use the /predict/ endpoint to upload images.\"} ''')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:42:04.618081Z","iopub.execute_input":"2025-05-12T21:42:04.618698Z","iopub.status.idle":"2025-05-12T21:42:04.624178Z","shell.execute_reply.started":"2025-05-12T21:42:04.618669Z","shell.execute_reply":"2025-05-12T21:42:04.623289Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!ngrok authtoken 2nevpSdeWmLxFqPvAHHTPDgNpGk_7mo6QCtKpevEuVRkiyFT8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:42:52.774304Z","iopub.execute_input":"2025-05-12T21:42:52.775119Z","iopub.status.idle":"2025-05-12T21:42:56.926918Z","shell.execute_reply.started":"2025-05-12T21:42:52.775086Z","shell.execute_reply":"2025-05-12T21:42:56.925744Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from pyngrok import ngrok\n\n# Start ngrok tunnel on port 8000\npublic_url = ngrok.connect(8000)\nprint(f\"Public URL: {public_url}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:43:07.426974Z","iopub.execute_input":"2025-05-12T21:43:07.427313Z","iopub.status.idle":"2025-05-12T21:43:07.655798Z","shell.execute_reply.started":"2025-05-12T21:43:07.427283Z","shell.execute_reply":"2025-05-12T21:43:07.654876Z"}},"outputs":[{"name":"stdout","text":"Public URL: NgrokTunnel: \"https://bf9a-34-151-77-179.ngrok-free.app\" -> \"http://localhost:8000\"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!uvicorn main:app --host 0.0.0.0 --port 8000\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:43:10.047733Z","iopub.execute_input":"2025-05-12T21:43:10.048107Z"}},"outputs":[{"name":"stdout","text":"\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m108\u001b[0m]\n\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n\u001b[32mINFO\u001b[0m:     Application startup complete.\n\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:8000\u001b[0m (Press CTRL+C to quit)\n\u001b[32mINFO\u001b[0m:     197.52.81.105:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mOPTIONS /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mPOST /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mOPTIONS /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mPOST /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mOPTIONS /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mPOST /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mOPTIONS /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mPOST /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mPOST /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mPOST /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n\u001b[32mINFO\u001b[0m:     197.43.34.183:0 - \"\u001b[1mPOST /predict/ HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"with open('app.py', 'w') as f:\n    f.write('''\nfrom fastapi import FastAPI, UploadFile, File\nfrom pydantic import BaseModel\nimport torch\nfrom PIL import Image\nimport io\nfrom torchvision import transforms\nfrom pyngrok import ngrok\n\napp = FastAPI()\n\n# Load the TorchScript model\nmodel = torch.jit.load(\"/kaggle/input/final_model/pytorch/default/1/model_final.pt\")\ndevice = torch.device(\"cpu\")  # Force to CPU\nmodel.to(device)\nmodel.eval()\n\n# Define the image preprocessing function\ndef get_transforms():\n    transform = transforms.Compose([\n        transforms.Resize((248, 248)),\n        transforms.ToTensor(),\n    ])\n    return transform\n\ndef preprocess_and_predict(model, img_tensor):\n    img_tensor = img_tensor.unsqueeze(0).to(device)  # Add batch dimension\n\n    # Compute mean and std\n    mean = torch.mean(img_tensor)\n    std = torch.std(img_tensor)\n\n    # Make prediction\n    with torch.no_grad():\n        output = model(img_tensor, mean.unsqueeze(0).to(device), std.unsqueeze(0).to(device))\n        predicted_class = output.argmax(dim=1).item()\n\n    return predicted_class, mean.item(), std.item()\n\n# Define the response model\nclass PredictionResponse(BaseModel):\n    predicted_class: int\n    mean: float\n    std: float\n    description: str\n\n@app.post(\"/predict/\", response_model=PredictionResponse)\nasync def predict_image(file: UploadFile = File(...)):\n    # Read image file\n    image_bytes = await file.read()\n    image = Image.open(io.BytesIO(image_bytes)).convert('L')\n\n    # Preprocess the image\n    transform = get_transforms()\n    img_tensor = transform(image)\n\n    # Run the prediction\n    predicted_class, mean, std = preprocess_and_predict(model, img_tensor)\n\n    # Map predicted class to dementia type\n    dementia_types = {0: \"Mild Dementia\", 1: \"Moderate Dementia\", 2: \"Non Demented\", 3: \"Very Mild Dementia\"}\n    description = dementia_types.get(predicted_class, \"Not Supported\")\n\n    return PredictionResponse(\n        predicted_class=predicted_class,\n        mean=mean,\n        std=std,\n        description=description\n    )\n\n# Add a root route\n@app.get(\"/\")\nasync def read_root():\n    return {\"message\": \"Welcome to the Dementia Prediction API. Use the /predict/ endpoint to upload images.\"}\n\n# Start ngrok tunnel on port 8000\nngrok.set_auth_token(\"2nevpSdeWmLxFqPvAHHTPDgNpGk_7mo6QCtKpevEuVRkiyFT8\")  # Replace with your actual auth token\npublic_url = ngrok.connect(8000)\nprint(f\"Public URL: {public_url}\")\n\n# Run the app using command line, outside this script:\n!uvicorn main:app --host 0.0.0.0 --port 8000\n''') ","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('main.py', 'w') as f:\n    f.write('''\nfrom fastapi import FastAPI, UploadFile, File\nfrom pydantic import BaseModel\nimport torch\nfrom PIL import Image\nimport io\nfrom torchvision import transforms\n\napp = FastAPI()\n\n# Load the TorchScript model\nmodel = torch.jit.load(\"/kaggle/input/final_model/pytorch/default/1/model_final.pt\")\ndevice = torch.device(\"cpu\")  # Force to CPU\nmodel.to(device)\nmodel.eval()\n\n# Define the image preprocessing function\ndef get_transforms():\n    transform = transforms.Compose([\n        transforms.Resize((248, 248)),\n        transforms.ToTensor(),\n    ])\n    return transform\n\ndef preprocess_and_predict(model, img_tensor):\n    img_tensor = img_tensor.unsqueeze(0).to(device)  # Add batch dimension\n\n    # Compute mean and std\n    mean = torch.mean(img_tensor)\n    std = torch.std(img_tensor)\n\n    # Make prediction\n    with torch.no_grad():\n        output = model(img_tensor, mean.unsqueeze(0).to(device), std.unsqueeze(0).to(device))\n        predicted_class = output.argmax(dim=1).item()\n\n    return predicted_class, mean.item(), std.item()\n\n# Define the response model\nclass PredictionResponse(BaseModel):\n    predicted_class: int\n    mean: float\n    std: float\n    description: str\n\n@app.post(\"/predict/\", response_model=PredictionResponse)\nasync def predict_image(file: UploadFile = File(...)):\n    # Read image file\n    image_bytes = await file.read()\n    image = Image.open(io.BytesIO(image_bytes)).convert('L')\n\n    # Preprocess the image\n    transform = get_transforms()\n    img_tensor = transform(image)\n\n    # Run the prediction\n    predicted_class, mean, std = preprocess_and_predict(model, img_tensor)\n\n    # Map predicted class to dementia type\n    dementia_types = {0: \"Mild Dementia\", 1: \"Moderate Dementia\", 2: \"Non Demented\", 3: \"Very Mild Dementia\"}\n    description = dementia_types.get(predicted_class, \"Not Supported\")\n\n    return PredictionResponse(\n        predicted_class=predicted_class,\n        mean=mean,\n        std=std,\n        description=description\n    )\n\n# Add a root route\n@app.get(\"/\")\nasync def read_root():\n    return {\"message\": \"Welcome to the Dementia Prediction API. Use the /predict/ endpoint to upload images.\"}\n\n\n\n''')\n    ","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}