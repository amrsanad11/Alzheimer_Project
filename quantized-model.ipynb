{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T13:47:44.858619Z","iopub.status.busy":"2024-10-14T13:47:44.858215Z","iopub.status.idle":"2024-10-14T13:47:51.665788Z","shell.execute_reply":"2024-10-14T13:47:51.664948Z","shell.execute_reply.started":"2024-10-14T13:47:44.858571Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/3840961475.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('/kaggle/input/model/pytorch/default/1/CNN_2024_10_13-15_18_42.pt'))\n"]}],"source":["import os\n","# from datetime import datetime\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n","# from sklearn.metrics import f1_score\n","from torchvision import datasets, transforms\n","# from tqdm import tqdm\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channel, mid_channel, out_channel):\n","        super().__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_channel, mid_channel, kernel_size=3, padding=1)\n","        self.batch_norm1 = nn.BatchNorm2d(mid_channel)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv2d(mid_channel, out_channel, kernel_size=3, padding=1)\n","        self.batch_norm2 = nn.BatchNorm2d(out_channel)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","    \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.batch_norm1(x)\n","        x = self.relu(x)\n","        \n","        x = self.conv2(x)\n","        x = self.batch_norm2(x)\n","        x = self.relu(x)\n","        \n","        x = self.pool(x)\n","        \n","        return x\n","class LinearBlock(nn.Module):\n","    def __init__(self, in_channel, out_channel):\n","        super().__init__()\n","        \n","        self.fc = nn.Linear(in_channel, out_channel)\n","        self.batch_norm = nn.BatchNorm1d(out_channel)\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","        x = self.fc(x)\n","        x = self.batch_norm(x)\n","        x = self.relu(x)\n","        \n","        return x\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.convblock1 = ConvBlock(1, 32, 64)\n","        self.convblock2 = ConvBlock(64, 128, 128)\n","        self.convblock3 = ConvBlock(128, 256, 256)\n","        self.convblock4 = ConvBlock(256, 512, 512)\n","        \n","        self.flatten = nn.Flatten(start_dim=1)\n","        \n","        self.linearblock1 = LinearBlock(512 * 15 * 15, 1024)\n","        self.linearblock2 = LinearBlock(1024, 512)\n","        self.linearblock3 = LinearBlock(512, 16)\n","        \n","        self.linearblock4 = LinearBlock(16 + 2, 4)\n","        self.softmax = nn.Softmax(dim=1)\n","    \n","    def forward(self, img, mean, std):\n","        x = self.convblock1(img)\n","        x = self.convblock2(x)\n","        x = self.convblock3(x)\n","        x = self.convblock4(x)\n","        \n","        x = self.flatten(x)\n","        \n","        x = self.linearblock1(x)\n","        x = self.linearblock2(x)\n","        x = self.linearblock3(x)\n","        \n","        x = torch.concat([x, mean.unsqueeze(1), std.unsqueeze(1)], dim=-1)\n","        x = self.linearblock4(x)\n","        x = self.softmax(x)\n","        \n","        return x\n","\n","    \n","model = CNN()\n","\n","# Load the state dictionary\n","model.load_state_dict(torch.load('/kaggle/input/model/pytorch/default/1/CNN_2024_10_13-15_18_42.pt'))\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Apply dynamic quantization\n","quantized_model = torch.quantization.quantize_dynamic(\n","    model, {torch.nn.Linear}, dtype=torch.qint8\n",")\n","\n","# Save the quantized model\n","torch.save(quantized_model, 'model_quantized.pt')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T13:52:07.438731Z","iopub.status.busy":"2024-10-14T13:52:07.437968Z","iopub.status.idle":"2024-10-14T13:52:09.360130Z","shell.execute_reply":"2024-10-14T13:52:09.359073Z","shell.execute_reply.started":"2024-10-14T13:52:07.438690Z"},"trusted":true},"outputs":[],"source":["# Define your mean and std (these are just example values; adjust as needed)\n","mean = torch.tensor([0.485])  # Mean for single channel\n","std = torch.tensor([0.229])    # Std for single channel\n","\n","# Create example input with 1 channel\n","example_input = torch.randn(1, 1, 240, 240)  # Shape: (batch_size, channels, height, width)\n","\n","# Trace the model, providing mean and std as additional inputs\n","traced_model = torch.jit.trace(quantized_model, (example_input, mean, std))\n","\n","# Save the traced model\n","traced_model.save('model_final.pt')\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T14:38:59.294977Z","iopub.status.busy":"2024-10-14T14:38:59.294564Z","iopub.status.idle":"2024-10-14T14:39:03.803961Z","shell.execute_reply":"2024-10-14T14:39:03.803137Z","shell.execute_reply.started":"2024-10-14T14:38:59.294938Z"},"trusted":true},"outputs":[],"source":["import torch.quantization\n","import os\n","# from datetime import datetime\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sn\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n","# from sklearn.metrics import f1_score\n","from torchvision import datasets, transforms\n","# from tqdm import tqdm\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channel, mid_channel, out_channel):\n","        super().__init__()\n","        \n","        self.conv1 = nn.Conv2d(in_channel, mid_channel, kernel_size=3, padding=1)\n","        self.batch_norm1 = nn.BatchNorm2d(mid_channel)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv2d(mid_channel, out_channel, kernel_size=3, padding=1)\n","        self.batch_norm2 = nn.BatchNorm2d(out_channel)\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","    \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.batch_norm1(x)\n","        x = self.relu(x)\n","        \n","        x = self.conv2(x)\n","        x = self.batch_norm2(x)\n","        x = self.relu(x)\n","        \n","        x = self.pool(x)\n","        \n","        return x\n","class LinearBlock(nn.Module):\n","    def __init__(self, in_channel, out_channel):\n","        super().__init__()\n","        \n","        self.fc = nn.Linear(in_channel, out_channel)\n","        self.batch_norm = nn.BatchNorm1d(out_channel)\n","        self.relu = nn.ReLU()\n","    \n","    def forward(self, x):\n","        x = self.fc(x)\n","        x = self.batch_norm(x)\n","        x = self.relu(x)\n","        \n","        return x\n","class CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.convblock1 = ConvBlock(1, 32, 64)\n","        self.convblock2 = ConvBlock(64, 128, 128)\n","        self.convblock3 = ConvBlock(128, 256, 256)\n","        self.convblock4 = ConvBlock(256, 512, 512)\n","        \n","        self.flatten = nn.Flatten(start_dim=1)\n","        \n","        self.linearblock1 = LinearBlock(512 * 15 * 15, 1024)\n","        self.linearblock2 = LinearBlock(1024, 512)\n","        self.linearblock3 = LinearBlock(512, 16)\n","        \n","        self.linearblock4 = LinearBlock(16 + 2, 4)\n","        self.softmax = nn.Softmax(dim=1)\n","    \n","    def forward(self, img, mean, std):\n","        x = self.convblock1(img)\n","        x = self.convblock2(x)\n","        x = self.convblock3(x)\n","        x = self.convblock4(x)\n","        \n","        x = self.flatten(x)\n","        \n","        x = self.linearblock1(x)\n","        x = self.linearblock2(x)\n","        x = self.linearblock3(x)\n","        \n","        x = torch.concat([x, mean.unsqueeze(1), std.unsqueeze(1)], dim=-1)\n","        x = self.linearblock4(x)\n","        x = self.softmax(x)\n","        \n","        return x\n","\n","    \n","model = CNN()\n","# Set the model to evaluation mode and prepare it for quantization\n","model.eval()\n","\n","# Fuse modules (if applicable)\n","# model.fuse_model()  # Uncomment if you have fusable layers like Conv2d and ReLU\n","\n","# Specify the quantization configuration\n","model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n","\n","# Prepare the model for quantization\n","torch.quantization.prepare(model, inplace=True)\n","\n","# Calibrate the model with representative data\n","# Here you would typically run some data through the model to collect statistics\n","# for example:\n","# for data in calibration_data_loader:\n","#     model(data)\n","\n","# Convert to a quantized model\n","quantized_model = torch.quantization.convert(model, inplace=True)\n","\n","# Save the quantized model\n","torch.save(quantized_model,'quantized_model_2.pt')\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T14:40:03.335425Z","iopub.status.busy":"2024-10-14T14:40:03.334500Z","iopub.status.idle":"2024-10-14T14:40:04.203525Z","shell.execute_reply":"2024-10-14T14:40:04.202488Z","shell.execute_reply.started":"2024-10-14T14:40:03.335381Z"},"trusted":true},"outputs":[],"source":["import torch.nn.utils.prune as prune\n","\n","# Apply pruning to your model layers\n","for layer in quantized_model.modules():\n","    if isinstance(layer, nn.Conv2d):\n","        prune.ln_structured(layer, name='weight', amount=0.2, n=2, dim=0)  # 20% pruning\n","\n","torch.save(quantized_model,'quantized_model_3.pt')"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-14T14:44:44.505197Z","iopub.status.busy":"2024-10-14T14:44:44.504464Z","iopub.status.idle":"2024-10-14T14:44:44.679552Z","shell.execute_reply":"2024-10-14T14:44:44.678622Z","shell.execute_reply.started":"2024-10-14T14:44:44.505155Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing preprocess_and_predict.py\n"]}],"source":["%%writefile preprocess_and_predict.py \n","from PIL import Image\n","\n","def get_transforms():\n","    transform = transforms.Compose([\n","        transforms.Resize((248, 248)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    return transform\n","\n","def preprocess_and_predict(model, image_path, device):\n","    \"\"\"\n","    Preprocess a single image and make a prediction using the trained model.\n","\n","    Args:\n","        model: The trained CNN model.\n","        image_path (str): Path to the image file.\n","        device: The device to run the model on (CPU or GPU).\n","\n","    Returns:\n","        predicted_class (int): The predicted class index.\n","        mean (float): The mean value of the image tensor.\n","        std (float): The standard deviation of the image tensor.\n","    \"\"\"\n","    # Load the image\n","    img = Image.open(image_path).convert('L')  # Convert image to grayscale\n","\n","    # Transform the image\n","    transform = get_transforms()\n","    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n","\n","    # Move tensor to the specified device\n","    img_tensor = img_tensor.to(device)\n","\n","    # Compute mean and std for the image\n","    mean = torch.mean(img_tensor)\n","    std = torch.std(img_tensor)\n","\n","    # Make a prediction\n","    model.eval()  # Set the model to evaluation mode\n","    with torch.no_grad():\n","        output = model(img_tensor, mean.unsqueeze(0).to(device), std.unsqueeze(0).to(device))  # Include mean and std in the prediction\n","        predicted_class = output.argmax(dim=1).item()  # Get the predicted class index\n","\n","    return predicted_class, mean.item(), std.item()\n","\n","# Example usage\n","image_path = '/kaggle/input/imagesoasis/Data/Very mild Dementia/OAS1_0003_MR1_mpr-1_102.jpg'\n","predicted_class_index, mean, std = preprocess_and_predict(traced_model, image_path, device)\n","\n","print(f\"Predicted Class Index: {predicted_class_index}, Mean: {mean}, Std: {std}\")\n","if predicted_class_index == 0 :\n","    print ('Mild Dementia')\n","    \n","elif predicted_class_index ==1 :\n","    print('Moderate Dementia')\n","    \n","elif predicted_class_index == 2 :\n","    print ('Non Demented')\n","    \n","elif predicted_class_index ==3 :\n","    print('Very mild Dementia')\n","    \n","else :\n","    print('not supported')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"source":["    def __init__(self, train_batch_size=64, test_batch_size=64, learning_rate=0.001, num_epochs=10, val_split=0.15, test_split=0.15, model_path='saved_model', dataset_path='/kaggle/input/imagesoasis/Data'):\n","        self.train_batch_size = train_batch_size\n","        self.test_batch_size = test_batch_size\n","        self.learning_rate = learning_rate\n","        self.num_epochs = num_epochs\n","        self.val_split = val_split\n","        self.test_split = test_split\n","        self.model_path = model_path\n","        self.dataset_path = dataset_path\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, subset, transform=None):\n","        self.subset = subset\n","        self.transform = transform\n","    \n","    def __getitem__(self, index):\n","        x, y = self.subset[index]\n","        if self.transform:\n","            x = self.transform(x)\n","            mean = torch.mean(x)\n","            std = torch.std(x)\n","        return x, mean, std, y\n","    \n","    def __len__(self):\n","        return len(self.subset)\n","def get_transforms():\n","    transform = transforms.Compose([\n","        transforms.Resize((248, 248)),\n","        transforms.ToTensor(),\n","    ])\n","\n","    return transform\n","def get_sample_weights(dataset, train_dataset):\n","    \n","    # Code taken from:\n","    #     https://www.maskaravivek.com/post/pytorch-weighted-random-sampler/\n","    y_train_indices = train_dataset.indices\n","    y_train = [dataset.targets[i] for i in y_train_indices]\n","    \n","    class_sample_counts = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n","    \n","    weights = 1. / class_sample_counts\n","    sample_weights = np.array([weights[t] for t in y_train])\n","    sample_weights = torch.from_numpy(sample_weights)\n","    \n","    return sample_weights\n","def get_data_loaders(hparams):\n","    # Loading the dataset\n","    dataset = datasets.ImageFolder(hparams.dataset_path,\n","                                   transform=transforms.Compose([transforms.Grayscale()]))\n","    \n","    # Splitting dataset into train, validation and test partitions.\n","    proportions = [(1 - hparams.val_split - hparams.test_split), hparams.val_split, hparams.test_split]\n","    lengths = [int(p * len(dataset)) for p in proportions]\n","    lengths[-1] = len(dataset) - sum(lengths[:-1])\n","    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, lengths)\n","    \n","    print(f'train size: {lengths[0]}, val size: {lengths[1]}, test size: {lengths[2]}')\n","    \n","    data_transforms = {\n","        'train': get_transforms(),\n","        'test': get_transforms()\n","    }\n","    \n","    # Using WeightedRandomSampler to overcome unbalance problem\n","    sample_weights = get_sample_weights(dataset, train_dataset)\n","    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights.type('torch.DoubleTensor'), len(sample_weights))\n","    \n","    train_dataset = CustomDataset(train_dataset, transform=data_transforms['train'])\n","    val_dataset = CustomDataset(val_dataset, transform=data_transforms['test'])\n","    test_dataset = CustomDataset(test_dataset, transform=data_transforms['test'])\n","    \n","    # Creating loaders\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams.train_batch_size, sampler=train_sampler, drop_last=True)\n","    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=hparams.train_batch_size, drop_last=True)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=hparams.test_batch_size)\n","\n","    return train_loader, val_loader, test_loader\n","\n","def predict(model, data_loader, criterion, device, eval=False):\n","    model.eval()\n","    pred_loss = 0\n","    pred_correct = 0\n","    total_size = 0\n","\n","    predictions = torch.IntTensor()\n","    ground_truths = torch.IntTensor()\n","\n","    predictions, ground_truths = predictions.to(device), ground_truths.to(device)\n","\n","    with torch.no_grad():\n","        for batch_idx, (img, mean, std, target) in enumerate(data_loader):\n","            img, mean, std, target = img.to(device), mean.to(device), std.to(device), target.to(device)\n","            output = model(img, mean, std)\n","            loss = criterion(output, target)\n","            pred_loss += loss.item()\n","            pred = output.argmax(dim=1, keepdim=True)\n","            pred_correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","            predictions = torch.cat((predictions, pred), dim=0)\n","            ground_truths = torch.cat((ground_truths, target), dim=0)\n","            \n","            total_size += len(img)\n","    \n","    pred_loss /= total_size\n","    pred_accuracy = 100. * pred_correct / total_size\n","\n","    if eval:\n","        return pred_loss, pred_accuracy, predictions.cpu().numpy(), ground_truths.cpu().numpy()\n","    else:\n","        return predictions.cpu().numpy(), ground_truths.cpu().numpy()\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3419493,"sourceId":5962731,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelId":138420,"modelInstanceId":115157,"sourceId":136098,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"myenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}
